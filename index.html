<!DOCTYPE html>
<html>
  <head>
    <title>ECE M202A Project</title>
  </head>
  <body>
    <h1 style="text-align: center; font-size: 40px;">
      <b>Head Pose Estimation using Earables</b>
    </h1>
    <h2>Literature Analysis</h2>
    <h3>Pre-trained Model for Head Pose Estimation from a Single Image using FSA-Net</h3>
    <p>
      We are going use the outputs of a pre-trained model as labels for our own model training. The vital method of this trained model is proposed for head pose estimation from a single image, a single RGB frame. The basic method is about regression and feature aggregation. The soft stagewise regression scheme is used to build the model. Moreover, this paper proposes to learn a fine-grained structure, which provides part-based information and pooled values, and spatially group pixel-level features of the feature map together into a set of features encoded with spatial information [1]. As a result, generated different variants can form complementary ensemble with learnable and non-learnable importance over the spatial location for more versatile spatial information [1].
      <br/><br/>By formulating the pose estimation, the soft stagewise regression and proposed FSA-Net are applied to pose estimation [1]. FSA-Net is composed of scoring function and the fine-grained structure mapping. Complementary model variants can be learned by defining learnable and non-learnable scoring functions of the pixel-level features [1]. Furthermore, there are implemented experiments showing that the ensemble of these variants outperforms the state-of-the-art methods.
      <br/><br/>To try this model demo, the required platforms are Keras, tensorflow, GPU and Ubuntu with some mandatory dependencies. For the demo, the center of the color axes is the detected face center. Ideally, each frame should have new face detection results. However, if the face detection fails, the previous detection results will be used to estimate poses. The SSD face detection ought to be fast and robust, and that’s what we would like to try and use first.
      <br/><br/>
    </p>
    <h3>Human Dynamic Estimation Using Recurrent Neural Networks with IMU Information</h3>
    <p>
      Since a period of IMU information collected from Esense Earables only provides a piece of information about human head dynamics. It cannot give a full image on head pose. For example, if the user stops moving for a while, the information collected from accelerometers and gyroscopes is not enough to estimate the user’s head pose. Thus, it is necessary for our estimation model to estimate the head pose based on both IMU information and the history of head poses. A recurrent neural network has internal state (memory) which can be used to record the previous head poses. We tried to find papers on Human pose estimation using IMU information only; however, most papers combined IMU and camera information to do head pose estimation.
      <br/><br/>We found several papers focused on Human Dynamics Estimation using Recurrent Neural Networks with IMU information. A Convolution neural network helps to extract key features from the temporal axis and the Recurrent neural network allows to take both current input and previous ones. Combining the two networks can generate better performance. The three papers focused on different human dynamics estimation and the evaluations of results use different standards. Thus, I summarized the devices, network structures and evaluations of the three papers in the following table.
    </p>
    <table>
      <tr>
        <th>Paper</th>
        <th>Human Dynamic Estimation</th>
        <th>Device</th>
        <th>Structure</th>
        <th>Evaluation</th>
      </tr>
      <tr>
        <td>[2]</td>
        <td>Human Hand Activities</td>
        <td>Accelerometer, Gyroscope, Magnetometer</td>
        <td>
          Two recurrent layers;
          <br/>Each one with 256 LSTM cells and a fully connect layer;
        </td>
        <td>Accuracy: 80.09%, 74.92% (noise)</td>
      </tr>
      <tr>
        <td>[3]</td>
        <td>Eating Patterns</td>
        <td>Piezoelectric sensors, Accelerometer, Gyroscope, Magnetometer</td>
        <td>
          SwallowNet:
          <br/>Savitzky-Golay filter (preprocessing);
          <br/>32 LSTM layers;
        </td>
        <td>F-score: 74.16</td>
      </tr>
      <tr>
        <td>[4]</td>
        <td>Dynamic Human Velocities</td>
        <td>Accelerometer, Gyroscope</td>
        <td>
          CNN-BLSTM network:
          <br/>1D convolutional layer (k=128) followed by a Batch normalization and a Rectified linear unit layer;
          <br/>2D convolutional layer followed by a Batch normalization and a Rectified linear unit layer;
          <br/>(un)folding layer and a flattening layer provides flat sequence
        </td>
        <td>ARMSE: 0.32</td>
      </tr>
    </table>
    <p>
      All the paper mentioned the noise brought by accelerometer; it is recommended to do data reprocessing on the IMU information, such as denoising and filtering. Paper [3] and [4] developed complex neural network structure for estimation. And paper [2] simply used two recurrent LSTM layers to extract features of IMU information. We would like to consider paper [2] as our primary deep learning model for head pose estimation.
    </p>
    <h3>References</h3>
    <ul>
      <li>
        [1] Tsun-Yi Yang, Yi-Ting Chen, Yen-Yu Lin, and Yung-Yu Chuang. FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation from a Single Image. In <i>Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2019. 
      </li>
      <li>
        [2] Patricio Rivera, Edwin Valarezo, Mun-Taek Choi, and Tae-Seong Kim. Recognition of Human Hand Activities Based on a Single Wrist IMU Using Recurrent Neural Networks. In <i>International Journal of Pharma Medicine and Biological Sciences Vol. 6, No. 4</i>, 2017
      </li>
      <li>
        [3] Dzung Tri Nguyen, Eli Cohen, Mohammad Pourhomayoun, and Nabil Alshurafa.SwallowNet. Recurrent Neural Network Detects and Characterizes Eating Patterns. In <i>Second IEEE PerCom Workshop on Pervasive Health Technologies</i>, 2017
      </li>
      <li>
        [4] Tobias Feigl, Sebastian Kram, Philipp Woller, Ramiz H. Siddiqui, Michael Philippsen, and Christopher Mutschler. A Bidirectional LSTM for Estimating Dynamic Human Velocities from a Single IMU. In <i>International Conference on Indoor Positioning and Indoor Navigation (IPIN)</i>, 2019
      </li>
    </ul>
    <h2>Project Summary</h2>
    <p>
      Objective:<br/><br/>The goal of this project is to estimate the head-pose using IMU information collected from Nokia’s eSense earables and use the estimated head-pose to create and show animojis for users on an android device.
      <br/><br/><br/>
      Envisioned approach:<br/><br/>First, we record facial videos and IMU information at the same time and then we generate ground truth tables using the videos with matured head-pose estimation algorithm. We plan to build head-pose estimation model using collected IMU information as training data and previously generated ground truth tables as the label of training data. We plan to use CNN (Convolutional Neural Network) to train the model. At the end, we will implement an android app to show animojis of users just using IMU information.
      <br/>
    </p>
    <p>Planned deliverables:</p>
    <ul>
      <li>Collected sample data: facial recordings and IMU information</li>
      <li>Ground truth tables generated from facial recordings</li>
      <li>CNN model for head-pose estimation</li>
      <li>Android app to create and show animojis for users</li>
    </ul>
    <p>Rough timeline:</p>
    <ul>
      <li>Week 4 – 6.5: Collecting sample data and generate ground truth tables</li>
      <li>Week 6.5 – 8: Train the RNN model for head-pose estimation</li>
      <li>Week 9– 10: implement the android app for animojis</li>
    </ul>
    <p>Work distribution:</p>
    <ul>
      <li>Collect sample data & generate ground truth table: Yuanyuan Xiang (50%), Qiuyang Yue (50%)</li>
      <li>RNN model for head pose estimation: Yuanyuan Xiang (60%), Qiuyang Yue (40%)</li>
      <li>Develop an Android app: Qiuyang Yue (60%), Yuanyuan Xiang (40%)</li>
    </ul>
    
  </body>
<html>
